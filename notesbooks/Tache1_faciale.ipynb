{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68721d81",
   "metadata": {},
   "source": [
    "\n",
    "# T√¢che 1 ‚Äî Reconnaissance Faciale (Matching)\n",
    "## 1. Introduction & Objectifs\n",
    "- Objectif : d√©terminer si deux images repr√©sentent la m√™me personne (matching).\n",
    "- Donn√©es : `data/train` (2000 personnes √ó 2 photos `XXXX_0`/`XXXX_1`), `data/test` (2000 images).\n",
    "- Livrables : \n",
    "  - `results/face_matching/face_matching_predictions.csv`\n",
    "  - `models/face_matching/face_matching_best.pth`\n",
    "  - Notebook complet (ce fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafbce7",
   "metadata": {},
   "source": [
    "## 2. Chargement des librairies & Configuration\n",
    "- Import des librairies (torch, torchvision, opencv‚Ä¶)\n",
    "- Lecture du fichier de config : `src/utils/config_t1.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "314d5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration T√¢che 1 charg√©e ‚úÖ\n",
      "FaceMatchingNet import√© ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# === Rendre src importable depuis notebooks/ ===\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "# === Config YAML ===\n",
    "import yaml\n",
    "with open(\"../src/utils/config_t1.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# === Imports mod√®le ===\n",
    "from src.models import FaceMatchingNet\n",
    "\n",
    "print(\"Configuration T√¢che 1 charg√©e ‚úÖ\")\n",
    "print(\"FaceMatchingNet import√© ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd3b79",
   "metadata": {},
   "source": [
    "## 3. Exploration du dataset\n",
    "- V√©rifier volumes, formats, exemples d‚Äôimages\n",
    "- Journaliser : nb images, nb personnes, distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853a7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for fname in os.listdir(root_dir):\n",
    "            if fname.endswith(\".jpg\") or fname.endswith(\".png\"):\n",
    "                self.samples.append(fname)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.samples[idx]\n",
    "        img_path = os.path.join(self.root_dir, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, fname\n",
    "\n",
    "\n",
    "# Configuration DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_loader_t1 = DataLoader(\n",
    "    FaceDataset(\"../data/tache1/train\", transform=transform),\n",
    "    batch_size=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader_t1 = DataLoader(\n",
    "    FaceDataset(\"../data/tache1/test\", transform=transform),\n",
    "    batch_size=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92404ced",
   "metadata": {},
   "source": [
    "## 4. Construction des paires (positives/n√©gatives)\n",
    "- D√©tection/alignement visage\n",
    "- Normalisation\n",
    "- Strat√©gie d‚Äô√©quilibrage des paires\n",
    "- R√®gles anti-fuite entre train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71d5aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224]) | Labels: tensor([0., 1., 0., 1., 0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# === Dataset de paires ===\n",
    "class FacePairsDataset(Dataset):\n",
    "    def __init__(self, pairs_file, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.pairs = []\n",
    "        with open(pairs_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                img1, img2, label = line.split(\",\")\n",
    "                self.pairs.append((img1, img2, int(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_name, img2_name, label = self.pairs[idx]\n",
    "        img1 = Image.open(os.path.join(self.root_dir, img1_name)).convert(\"RGB\")\n",
    "        img2 = Image.open(os.path.join(self.root_dir, img2_name)).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        return (img1, img2), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# === Chemins fixes ===\n",
    "TRAIN_IMAGES = \"../data/tache1/train\"\n",
    "TEST_IMAGES  = \"../data/tache1/test\"\n",
    "TRAIN_PAIRS  = \"../data/tache1/train_pairs.csv\"\n",
    "TEST_PAIRS   = \"../data/tache1/test_pairs.csv\"\n",
    "\n",
    "# === Transformations ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === DataLoaders ===\n",
    "train_loader_t1 = DataLoader(\n",
    "    FacePairsDataset(TRAIN_PAIRS, TRAIN_IMAGES, transform=transform),\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader_t1 = DataLoader(\n",
    "    FacePairsDataset(TEST_PAIRS, TEST_IMAGES, transform=transform),\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# === Test rapide ===\n",
    "batch = next(iter(train_loader_t1))\n",
    "(img1, img2), labels = batch\n",
    "print(\"Shapes:\", img1.shape, img2.shape, \"| Labels:\", labels[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87294832",
   "metadata": {},
   "source": [
    "## 5. Mod√®le\n",
    "- D√©finition du Dataset pour paires\n",
    "- Param√®tres : batch_size, seed\n",
    "- Choix : embeddings visage + mesure de similarit√© (cosine)\n",
    "- Perte : contrastive/triplet\n",
    "- Sauvegarde des poids dans `models/face_matching/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "498866fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device utilis√© : mps\n",
      "[Epoch 1/5] Train Loss: 0.6936 | Val Loss: 0.6931 | Val Acc: 0.5000\n",
      "üíæ ‚úÖ Meilleur mod√®le sauvegard√© !\n",
      "[Epoch 2/5] Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000\n",
      "[Epoch 3/5] Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000\n",
      "[Epoch 4/5] Train Loss: 0.6932 | Val Loss: 0.6931 | Val Acc: 0.5000\n",
      "[Epoch 5/5] Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# === Device ===\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Device utilis√© : {device}\")\n",
    "\n",
    "# === Mod√®le (doit √™tre d√©fini/import√© avant) ===\n",
    "model = FaceMatchingNet().to(device)\n",
    "\n",
    "# === Perte + Optimizer ===\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config[\"training\"][\"learning_rate\"]\n",
    ")\n",
    "\n",
    "# Seuil pour la pr√©diction\n",
    "threshold = float(config.get(\"evaluation\", {}).get(\"threshold_similarity\", 0.5))\n",
    "\n",
    "# === Fonctions utilitaires ===\n",
    "def train_one_epoch(loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for (img1, img2), labels in loader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(img1, img2).view(-1)   # [B] ou [B,1] ‚Üí aplati\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, model, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    for (img1, img2), labels in loader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        logits = model(img1, img2).view(-1)\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold).long()\n",
    "        correct += (preds == labels.long()).sum().item()\n",
    "        total   += labels.numel()\n",
    "\n",
    "    acc = correct / total if total else 0.0\n",
    "    return total_loss / max(1, len(loader)), acc\n",
    "\n",
    "# === Entra√Ænement global ===\n",
    "MODEL_DIR = os.path.join(\"..\", config[\"paths\"][\"output_models\"])\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "best_acc = 0.0\n",
    "num_epochs = int(config[\"training\"][\"epochs\"])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(train_loader_t1, model, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(val_loader_t1, model, criterion, device, threshold)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"best.pth\"))\n",
    "        print(\"üíæ ‚úÖ Meilleur mod√®le sauvegard√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e840b",
   "metadata": {},
   "source": [
    "## 6. Entra√Ænement\n",
    "- Boucle train/val\n",
    "- Journal des m√©triques par epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "708e9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pr√©dictions sauvegard√©es dans : ../results/face_matching/val_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from src.models.face_matching_model import FaceMatchingNet\n",
    "\n",
    "# === Dataset bas√© sur un CSV de paires ===\n",
    "class FacePairsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img1_path = os.path.join(self.root_dir, row[\"img1\"])\n",
    "        img2_path = os.path.join(self.root_dir, row[\"img2\"])\n",
    "        label = int(row.get(\"label\", 0))  # 0 = different, 1 = same (si pas pr√©sent)\n",
    "\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return (img1, img2), label\n",
    "\n",
    "# === Paths ===\n",
    "VAL_PAIRS = \"../results/face_matching/face_matching_pairs.csv\"  # ‚ö†Ô∏è Ton fichier g√©n√©r√©\n",
    "IMG_DIR   = \"../\" + config[\"paths\"][\"test_dir\"]                 # ex: ../data/tache1/test\n",
    "OUT_DIR   = \"../\" + config[\"paths\"][\"output_results\"]\n",
    "MODEL_PATH= \"../\" + config[\"paths\"][\"output_models\"] + \"/best.pth\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Device ===\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# === Model ===\n",
    "model = FaceMatchingNet().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Transform ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === Dataset & Loader ===\n",
    "val_ds = FacePairsDataset(VAL_PAIRS, IMG_DIR, transform=transform)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Pr√©dictions ===\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for (img1, img2), labels in val_loader:\n",
    "        img1, img2 = img1.to(device), img2.to(device)\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        outputs = model(img1, img2)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "        for true, pred in zip(labels, preds):\n",
    "            results.append({\"label\": true, \"prediction\": pred})\n",
    "\n",
    "# === Sauvegarde ===\n",
    "csv_path = os.path.join(OUT_DIR, \"val_predictions.csv\")\n",
    "pd.DataFrame(results).to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Pr√©dictions sauvegard√©es dans :\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dea67a",
   "metadata": {},
   "source": [
    "## 7. √âvaluation (Validation)\n",
    "- M√©triques : accuracy, precision, recall, F1\n",
    "- Matrice de confusion\n",
    "- Courbes d‚Äôapprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ad9a4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5550\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanszigui/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Charger r√©sultats val\n",
    "val_csv = \"../results/face_matching/val_predictions.csv\"\n",
    "df = pd.read_csv(val_csv)\n",
    "\n",
    "# Nettoyer la colonne prediction : \"[0]\" -> 0, \"[1]\" -> 1\n",
    "df[\"prediction\"] = df[\"prediction\"].astype(str).str.strip(\"[]\").astype(int)\n",
    "\n",
    "# Label est d√©j√† bon\n",
    "y_true = df[\"label\"].astype(int)\n",
    "y_pred = df[\"prediction\"].astype(int)\n",
    "\n",
    "# M√©triques\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec  = recall_score(y_true, y_pred)\n",
    "f1   = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782b9bf",
   "metadata": {},
   "source": [
    "## 8. Inf√©rence sur Test\n",
    "- G√©n√©rer `results/face_matching/face_matching_predictions.csv`\n",
    "- Format strict demand√© par l‚Äôorganisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5838b76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Appariements produits: 1000 paires\n",
      "üíæ Fichier √©crit: ../results/face_matching/face_matching_pairs.csv\n",
      "‚úÖ Fichier de soumission g√©n√©r√© : ../results/face_matching/face_matching_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# === Chemins & pr√©requis ===\n",
    "TEST_DIR = os.path.join(\"..\", config[\"paths\"][\"test_dir\"])               # ex: ../data/tache1/test\n",
    "OUT_DIR  = os.path.join(\"..\", config[\"paths\"][\"output_results\"])         # ex: ../results/face_matching\n",
    "WEIGHTS  = os.path.join(\"..\", config[\"paths\"][\"output_models\"], \"best.pth\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Dataset test simple (image, filename) ===\n",
    "class FaceTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.files = [f for f in os.listdir(root) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "        self.files.sort()\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        img   = Image.open(os.path.join(self.root, fname)).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, fname\n",
    "\n",
    "# === Transform identique √† l'entra√Ænement ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === Device & mod√®le ===\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = FaceMatchingNet().to(device)\n",
    "assert os.path.isfile(WEIGHTS), f\"Poids introuvables: {WEIGHTS}\"\n",
    "model.load_state_dict(torch.load(WEIGHTS, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Embeddings pour toutes les images du test ===\n",
    "test_ds  = FaceTestDataset(TEST_DIR, transform=transform)\n",
    "test_dl  = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "filenames, embs = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, fnames in test_dl:\n",
    "        imgs = imgs.to(device)\n",
    "        # On utilise la branche d'embedding (forward_once)\n",
    "        feats = model.forward_once(imgs) if hasattr(model, \"forward_once\") else model.embedding_net(imgs)\n",
    "        feats = F.normalize(feats, p=2, dim=1)  # normalisation L2\n",
    "        embs.append(feats.cpu())\n",
    "        filenames.extend(list(fnames))\n",
    "\n",
    "embs = torch.cat(embs, dim=0)  # [N, D]\n",
    "N    = embs.size(0)\n",
    "\n",
    "# === Matrice de similarit√© cosinus ===\n",
    "# sim[i,j] ~ proximit√©; on met diag √† -inf pour √©viter l'auto-matching\n",
    "sim = embs @ embs.T\n",
    "sim.fill_diagonal_(-float(\"inf\"))\n",
    "\n",
    "# === Appariement greedy 1-1 (on forme ~N/2 paires uniques avec score d√©croissant) ===\n",
    "pairs = []\n",
    "matched = set()\n",
    "# R√©cup√®re toutes les paires (i<j) tri√©es par similarit√© d√©croissante, sans construire une liste g√©ante\n",
    "# On proc√®de par it√©rations: pour chaque i non appari√©, on prend son meilleur j non appari√©\n",
    "for _ in range(N // 2):\n",
    "    # Trouver l'indice i non match√© avec meilleure piste\n",
    "    best_i, best_j, best_s = None, None, -1.0\n",
    "    for i in range(N):\n",
    "        if i in matched: \n",
    "            continue\n",
    "        # meilleur j pour cet i\n",
    "        s, j = torch.max(sim[i], dim=0)\n",
    "        s = s.item(); j = j.item()\n",
    "        if j in matched:\n",
    "            # on tente de trouver le meilleur j non match√© pour i\n",
    "            # (fallback rapide): on met sim[i, j] √† -inf et on retente 3 fois\n",
    "            tries = 0\n",
    "            while j in matched and tries < 3:\n",
    "                sim[i, j] = -float(\"inf\")\n",
    "                s, j = torch.max(sim[i], dim=0)\n",
    "                s = s.item(); j = j.item()\n",
    "                tries += 1\n",
    "        if j not in matched and s > best_s:\n",
    "            best_s = s; best_i = i; best_j = j\n",
    "    if best_i is None or best_j is None:\n",
    "        break\n",
    "    pairs.append((filenames[best_i], filenames[best_j], float(best_s)))\n",
    "    # On invalide ces deux lignes/colonnes pour ne plus les r√©utiliser\n",
    "    matched.add(best_i); matched.add(best_j)\n",
    "    sim[best_i, :] = -float(\"inf\"); sim[:, best_i] = -float(\"inf\")\n",
    "    sim[best_j, :] = -float(\"inf\"); sim[:, best_j] = -float(\"inf\")\n",
    "\n",
    "# === Sauvegarde CSV (img1,img2,score) ===\n",
    "out_csv = os.path.join(OUT_DIR, \"face_matching_pairs.csv\")\n",
    "with open(out_csv, \"w\") as f:\n",
    "    f.write(\"img1,img2,similarity\\n\")\n",
    "    for a,b,s in pairs:\n",
    "        f.write(f\"{a},{b},{s:.6f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Appariements produits: {len(pairs)} paires\")\n",
    "print(f\"üíæ Fichier √©crit: {out_csv}\")\n",
    "# === Conversion en format attendu pour soumission ===\n",
    "import pandas as pd\n",
    "\n",
    "pairs_csv = os.path.join(OUT_DIR, \"face_matching_pairs.csv\")\n",
    "df = pd.read_csv(pairs_csv)\n",
    "\n",
    "# Appliquer un seuil sur la similarit√© pour produire un label binaire (0=different, 1=m√™me personne)\n",
    "df[\"label\"] = (df[\"similarity\"] > 0.5).astype(int)\n",
    "\n",
    "# Garder uniquement les colonnes demand√©es\n",
    "submit_csv = os.path.join(OUT_DIR, \"face_matching_predictions.csv\")\n",
    "df[[\"img1\",\"img2\",\"label\"]].to_csv(submit_csv, index=False)\n",
    "\n",
    "print(f\"‚úÖ Fichier de soumission g√©n√©r√© : {submit_csv}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
